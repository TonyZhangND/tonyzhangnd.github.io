<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://tonyzhangnd.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tonyzhangnd.github.io/" rel="alternate" type="text/html" /><updated>2025-10-28T22:07:38-04:00</updated><id>https://tonyzhangnd.github.io/feed.xml</id><title type="html">Tony Zhang</title><subtitle>Personal website of Tony Zhang</subtitle><author><name>Tony Zhang</name><email>nudzhang@umich.edu</email></author><entry><title type="html">(Verifiable) AI is a Compiler</title><link href="https://tonyzhangnd.github.io/2025/10/AI-is-a-compiler.html" rel="alternate" type="text/html" title="(Verifiable) AI is a Compiler" /><published>2025-10-27T00:00:00-04:00</published><updated>2025-10-27T00:00:00-04:00</updated><id>https://tonyzhangnd.github.io/2025/10/AI-is-a-compiler</id><content type="html" xml:base="https://tonyzhangnd.github.io/2025/10/AI-is-a-compiler.html"><![CDATA[<p>Modern computers are magic rocks, and programming is the art of making magic rocks do useful things.
The challenge is that magic rocks only understand magic rock language, i.e., machine code such as x86 or ARM.
Magic rock language is far too cryptic for humans, so clever humans invented high-level programming languages such as C++ and Python.
Compilers, then, are the translators that turn those high-level languages into machine code.</p>

<p><img src="../../assets/images/verifiable-ai-compiler/compiler.png" alt="a compiler" />
<em>Compiler as a human-to-rock translator.</em></p>

<p>From the compiler’s perspective, a high-level program is a specification of what machine code to generate.
The key correctness property that a compiler must satisfy is that any such translation is always faithful – any two compiler invocations with the same input program must produce semantically equivalent machine code that fulfil the specification.</p>

<h2 id="programs-as-imperative-specifications">Programs as Imperative Specifications</h2>

<p>To make that job tractable, compilers impose restrictions on what kinds of inputs they accept.
In particular, the human programmer must describe precisely <em>how</em> a computation is performed.</p>

<p>For instance, in an imperative language like C++, the programmer can’t simply declare that they want the <em>n-th</em> Fibonacci number. Instead, they must explicitly specify control flow and memory updates.
Functional languages such as OCaml are more declarative about control flow and state, but they are still imperative about algorithmic intent.</p>

<p>In short, even though they are more expressive, today’s high-level languages are still imperative at heart<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.
They resemble machine code far more than they resemble natural language.</p>

<h2 id="are-llms-compilers">Are LLMs Compilers?</h2>

<p>It’s now 2025, the year of vide coding.
Rather than write C++, we describe what we want in natural language:
“Build a service that transfers money between accounts, ensuring balances never go negative.”
Seconds later, the code appears.</p>

<p>Technically, the LLM outputs a high-level programming language like C++, which is then passed to a traditional compiler. But conceptually, that step is incidental. From a wider perspective, the LLM here is a key part of the compiler toolchain that translates human intent into machine language.</p>

<p>The problem is that these new “LLM compilers” violate the most fundamental contract of any real compiler: semantic faithfulness.
LLMs are probabilistic. The same prompt, run twice on a given LLM, can produce different code, with neither version guaranteed to match the user’s intent. It might produce code that looks plausible but behaves incorrectly.</p>

<h2 id="verifiable-reasoning-llms-are-compilers">Verifiable Reasoning LLMs are Compilers</h2>

<p>This is where formal verification step in. It lets us prove mathematically that a program satisfies its specification.</p>

<p>If we couple LLMs with verifiable reasoning, every piece of generated code would come with a proof that it satisfies the intended property.
In the money-transfer example, the proof might state and verify that balances never go negative, even under concurrency and failure.</p>

<p>The result is <strong>verifiable program synthesis</strong>, a paradigm where the model not only writes code but also produces the evidence that the code is correct. 
Critically, AI is also perfectly suited to find such evidence, which I explained in a <a href="https://tonyzhangnd.github.io/2025/08/AI-for-verification.html">previous post</a>.</p>

<p>In this new interface, programmers no longer write imperative instructions for how to compute.
Instead, they write <strong>declarative specifications</strong> of what must hold<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>.
The LLM serves as a reasoning engine that searches for implementations consistent with those specifications.</p>

<h2 id="a-new-programming-paradigm">A New Programming Paradigm</h2>

<p>Verifiable program synthesis lifts programming to a higher level of abstraction: programmers describe intent, the model synthesizes candidates, and formal verification forbids any incorrect results.</p>

<p>This does not mean that algorithmic thinking is obsolete.
Programmers can, and indeed should, tell the LLM what algorithms or data structures to use, e.g., LRU or LFU policy for cache eviction, Paxos or Raft for state machine replication. But they’ll do so declaratively, by expressing the desired properties rather than wiring the mechanics.
The LLM-compiler stack then takes care of correctness.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>Of course, I’m omitting declarative but domain-specific languages such as SQL. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>These declarative specifications can either be in natural language or mathematics, but I will leave that discussion for another day. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Tony Zhang</name><email>nudzhang@umich.edu</email></author><category term="verification" /><category term="formal methods" /><category term="artificial intelligence" /><summary type="html"><![CDATA[When LLMs meet formal methods, AI code generation becomes provably correct.]]></summary></entry><entry><title type="html">Is AI the Catalyst for Practical Software Verification?</title><link href="https://tonyzhangnd.github.io/2025/08/AI-for-verification.html" rel="alternate" type="text/html" title="Is AI the Catalyst for Practical Software Verification?" /><published>2025-08-31T00:00:00-04:00</published><updated>2025-08-31T00:00:00-04:00</updated><id>https://tonyzhangnd.github.io/2025/08/AI-for-verification</id><content type="html" xml:base="https://tonyzhangnd.github.io/2025/08/AI-for-verification.html"><![CDATA[<p>Formal verification for software systems has been a hot topic in systems research over the past decade. The last two OSDI conferences have yielded <em>four</em> best-paper awards for work applying formal verification to real systems. The reason for this popularity is clear. Verification lets developers write programs that are bug-free by construction, determined at <em>compile time</em>. This is in contrast to relying on flaky unit tests, cumbersome integration tests, and cryptic logs that frustrate developers.</p>

<p>However, industry adoption of verification remains limited. Beyond <a href="https://www.amazon.science/research-areas/automated-reasoning">AWS</a> and <a href="https://www.mongodb.com/company/research/distributed-systems-research-group">MongoDB</a>, few tech companies actively use verification in their development process. Even when verification is used, it typically offered as a bespoke service led by researchers, rather than a tool accessible to everyday developers. Existing verification tools are simply too restrictive in their domain, or demand such specialized knowledge and manual effort that the return on investment is unjustifiable for most businesses.</p>

<p>Enter AI. With the explosion of coding assistants, one natural question to ask is: can artificial intelligence help crack the verification adoption barrier? I think the answer is likely yes.</p>

<p>In <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a>, Richard Sutton presents how in problems like chess and Go, general search and learning-based AI methods that leverage computational scaling laws have consistently outperformed approaches that rely on encoding human domain knowledge about the structure of these games. The same story has played out in domains such as speech recognition and computer vision.</p>

<p>Verification has striking parallels. Similar to chess and Go, it can be characterized as a search problem. Given a target program as input, and a specification, the goal is to find a <em>proof</em> string that passes a black-box verifier, i.e., the proof “convinces” a verifier oracle that the target program satisfies the specification. Some form of proof is always needed to guide the verifier, because the problem of determining if a program satisfies the specification is fundamentally undecidable.</p>

<div class="language-cs highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Target program we want to prove: return a </span>
<span class="c1">// sorted copy of arr</span>
<span class="n">function</span> <span class="nf">sort</span><span class="p">(</span><span class="n">arr</span><span class="p">:</span> <span class="n">seq</span><span class="p">&lt;</span><span class="kt">int</span><span class="p">&gt;)</span> <span class="p">:</span> <span class="n">seq</span><span class="p">&lt;</span><span class="kt">int</span><span class="p">&gt;</span> <span class="p">{</span>
    <span class="c1">// ...</span>
<span class="p">}</span>

<span class="c1">// Specification: sort function produces a </span>
<span class="c1">// sorted copy of any input</span>
<span class="n">predicate</span> <span class="nf">sortIsCorrect</span><span class="p">(</span><span class="n">arr</span><span class="p">:</span> <span class="n">seq</span><span class="p">&lt;</span><span class="kt">int</span><span class="p">&gt;)</span> <span class="p">{</span>
   <span class="nf">multiset</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="p">==</span>  <span class="nf">multiset</span><span class="p">(</span><span class="nf">sort</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span>
   <span class="p">&amp;&amp;</span> <span class="n">forall</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="p">::</span> <span class="n">i</span> <span class="p">&lt;</span> <span class="n">j</span> <span class="p">==&gt;</span> <span class="nf">sort</span><span class="p">(</span><span class="n">arr</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span> <span class="p">&lt;=</span> <span class="nf">sort</span><span class="p">(</span><span class="n">arr</span><span class="p">)[</span><span class="n">j</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1">// Proof that our target program is correct, </span>
<span class="c1">// over all possible inputs</span>
<span class="n">lemma</span> <span class="nf">sortIsCorrectProof</span><span class="p">()</span> 
    <span class="n">ensures</span> <span class="n">forall</span> <span class="n">arr</span> <span class="p">::</span> <span class="nf">sortIsCorrect</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">// &lt; Proof code here &gt;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Today, we rely on developers to manually derive this proof. Hence, the practicality of verification boils down to the ease of finding this string. Current research uses rule-based approaches to narrow the search space that human developers must explore. For instance, tools like <a href="https://www.usenix.org/conference/osdi25/presentation/zhang-tony">Basilisk (OSDI’25)</a> leverage certain recurring structures in these proofs to automatically generate proof fragments, but still require heavy human input to help compose the fragments into a final whole. In fact, it is unlikely we can invent a rule-based algorithm to fully automate proof derivation. The problem is too sensitive to the structure and correctness properties of the target program, and the whims of the underlying black-box verifier.</p>

<p>While difficult for humans, such a problem is perfect for AI. The verifier oracle provides a clear, binary objective function: a proof that passes the verifier is correct, while one that fails is incorrect. This eliminates the ambiguity that plagues many AI applications. Unlike code generation where correctness can be subjective, verification provides a definitive oracle that the AI cannot “hallucinate” around. The AI is free to explore the search space, but it cannot produce a misleadingly convincing but incorrect result.</p>

<p>Moreover, proof code is <em>untrusted code</em> that serves only as input to the verifier, and does not require human review. Once the verifier accepts it, the proof’s job is done. This means AI can produce sloppy proofs without much practical downside, as long as they pass verification.</p>

<p>We’re still in the early days of software verification, and I’m incredibly excited about what’s coming once the power of AI and computational scaling is brought to bear. AI might just be the missing catalyst that tips it from niche research to standard practice.</p>]]></content><author><name>Tony Zhang</name><email>nudzhang@umich.edu</email></author><category term="verification" /><category term="formal methods" /><category term="artificial intelligence" /><summary type="html"><![CDATA[Formal verification has always been powerful but impractical. AI might be the catalyst that finally makes it mainstream.]]></summary></entry><entry><title type="html">Consensus Protocol Using TLA+</title><link href="https://tonyzhangnd.github.io/2018/01/Consensus-TLA-Spec.html" rel="alternate" type="text/html" title="Consensus Protocol Using TLA+" /><published>2018-01-18T00:00:00-05:00</published><updated>2018-01-18T00:00:00-05:00</updated><id>https://tonyzhangnd.github.io/2018/01/Consensus%20TLA%20Spec</id><content type="html" xml:base="https://tonyzhangnd.github.io/2018/01/Consensus-TLA-Spec.html"><![CDATA[<aside class="sidebar__right">
<nav class="toc">
    <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Table of Contents</h4></header>
<ul class="toc__menu" id="markdown-toc">
  <li><a href="#consensus" id="markdown-toc-consensus">Consensus</a></li>
  <li><a href="#tla-model" id="markdown-toc-tla-model">TLA+ Model</a>    <ul>
      <li><a href="#state-machines-and-mathematical-functions" id="markdown-toc-state-machines-and-mathematical-functions">State Machines and Mathematical Functions</a></li>
      <li><a href="#init" id="markdown-toc-init">Init</a></li>
      <li><a href="#next" id="markdown-toc-next">Next</a></li>
    </ul>
  </li>
  <li><a href="#checking-for-correctness" id="markdown-toc-checking-for-correctness">Checking for Correctness</a></li>
  <li><a href="#concluding-remarks" id="markdown-toc-concluding-remarks">Concluding Remarks</a></li>
</ul>

  </nav>
</aside>

<p>Looking to dive into distributed systems research, I recently started learning how to write TLA+ specifications. <a href="https://lamport.azurewebsites.net/tla/tla.html">TLA+ is a specification language</a> used to mathematically describe an algorithm, and is adopted in both <a href="https://syslab.cs.washington.edu/papers/tapir-tr-v2.pdf">academia</a> and <a href="http://lamport.azurewebsites.net/tla/amazon.html">industry</a>. It provides functionalities to formally verify algorithm correctness. Precisely, it allows us to check if an algorithm satisfies the properties we want it to satisfy.</p>

<p>In this post I shall discuss a TLA+ specification of Consensus I wrote.   This specification describes the properties of Consensus at its highest level. That is, we are not concerned with <em>how</em> a specific protocol can achieve Consensus, but <em>what</em> properties should any solution to Consensus possess. It is thus useful for now to forget about mechanisms such as message-passing; a system that solves Consensus using extrasensory perception solves it equivalently as one using TCP.</p>

<h2 id="consensus">Consensus</h2>
<p>The Consensus problem is easy to state and understand. Yet it is the basic building-block of computer systems requiring coordination between agents.</p>

<p>Fundamentally, a protocol that solves Consensus in a synchronous system permitting crash failures must have the following properties.</p>

<ol>
  <li><strong>Validity:</strong> If all processes that propose a value propose \(v\), then all correct processes
eventually decide \(v\).</li>
  <li><strong>Agreement:</strong> If a correct process decides \(v\), then all correct processes eventually decide \(v\).</li>
  <li><strong>Integrity:</strong> If a correct process decides \(v\), then some process must have proposed \(v\).</li>
  <li><strong>Termination:</strong> Every correct process eventually decides some value.</li>
</ol>

<p>Agreement and Integrity are <strong>safety properties</strong>: they dictate that inconsistent decisions cannot occur. Validity and Termination are <strong>liveness properties</strong>: they prevent processes from trivially deciding, say, 0 in all cases, and from not halting.</p>

<p>For our present purpose let us consider, <a href="https://www.sciencedirect.com/science/article/pii/S0020019000000272">without loss of generality</a>, Binary Consensus, in which each participant proposes either 0 or 1.</p>

<h2 id="tla-model">TLA+ Model</h2>
<h3 id="state-machines-and-mathematical-functions">State Machines and Mathematical Functions</h3>
<p>To model Consensus mathematically as in TLA+, we adopt a state machine approach. Each participant in the protocol undergoes the following state transitions:</p>

<p><img src="/assets/images/consensus/consensusStateMachine.png" alt="" /></p>

<p>All processes begin in the working state. They then each propose a value, either 0 or 1, to the system. After which, all correct processes then deterministically decides on one of the proposed values (we do not yet care how).</p>

<p>To express these state transitions as math, we begin by defining some constants and variables. In TLA+, we write</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CONSTANTS PCS  \* The set of all processes

VARIABLES   states,     \* states of each process in PCS
            proposals,  \* proposals of each process in PCS
            pset,       \* set of all proposals 
            decisions   \* decision of each process in PCS
</code></pre></div></div>

<p>\(PCS\) and \(pset\) are mathematical sets. It may be intuitive at first to think of the variables \(states\), \(proposals\) and \(decisions\) as arrays in a programming language, mapping each process id to its respective state, proposal and decision. However, such intuition is not correct. TLA+ does not operate in the realm of memory addresses. The language of TLA+ is mathematics, and in math, arrays are functions with domain \(\mathbb{N}\) (the natural numbers).</p>

<p>But unlike arrays, TLA+ functions do not restrict us to any particular domain. Here, I shall define \(states\), \(proposals\) and \(decisions\) as functions mapping \(PCS\), the set of all processes, to their respective state, proposal and decision values.</p>

<h3 id="init">Init</h3>
<p>At the heart of all state machines, and indeed any TLA+ spec, are statements describing the initial state of the system, and how it transitions from one state to the next. In my model, while each process is a state machine, the entire system as a whole is itself a state machine, and it is this global state machine we describe in TLA+. Therefore, we can think of the behavior Consensus, and indeed any system, as a sequence of state transitions</p>

\[S_0 \rightarrow S_1 \rightarrow S_2 \rightarrow S_3 \rightarrow ...\]

<p>In TLA+, we define \(init\) as a boolean formula describing the properties of the initial state \(S_0\). I describe the initial state of the Consensus as so:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>init == /\ states = [p \in PCS |-&gt; "working"]
        /\ proposals \in [PCS -&gt; {0, 1}]
        /\ pset = {}
        /\ decisions = [p \in PCS |-&gt; -1]
</code></pre></div></div>
<p>This is a logical conjunct of four statements</p>

<ol>
  <li>\(states\) is the function \(states : PCS \rightarrow \{``working", ``proposed", ``decided"\}\) such that \(\forall p \in PCS : states(p) \mapsto ``working"\).</li>
  <li>\(proposals\) is some function \(proposals : PCS \rightarrow \{0, 1\}\). TLA+ will then iterate through all such functions.</li>
  <li>\(pset\) is the empty set.</li>
  <li>\(decisions\) is the function \(decisions : PCS \rightarrow \{-1, 0, 1\}\) such that \(\forall p \in PCS : decisions(p) \mapsto -1\), with \(-1\) as the placeholder for not having yet decided.</li>
</ol>

<p>Note that in each statement, we are not performing variable assignments. Instead, statements 1 through 4 are boolean statements. For instance, statement 1 asserts that \(states\) is indeed the function described. And thus, the statement we call \(init\) is a boolean statement asserting the properties of the initial state of Consensus – any system at its initial state \(S_0\) satisfies the initial spec of Consensus if and only if \(init\) is true when applied to \(S_0\).</p>

<h3 id="next">Next</h3>
<p>While \(init\) asserts what must be true in \(S_0\), the \(next\) formula describes the state transition</p>

\[S_i \rightarrow S_{i+1}\]

<p>Thus intuitively, if \(init\) is the base case for an inductive proof, then \(next\) is the inductive step. If \(init\) is true on \(S_0\), and \(next\) is true on \(S_i \rightarrow S_{i+1}\) for all \(i = 0, 1, 2, 3...\), then our system behaves correctly as specified.</p>

<p>For my Consensus spec, I split \(next\) into two possible actions of the system –  any process can either choose to propose a value (if it has not yet done so), or decide a value (after all processes have proposed). I write \(propose\) as so:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>propose(p) == /\ states[p] = "working"
              /\ states' = [states EXCEPT ![p] = "proposed"]
              /\ pset' = pset \cup {proposals[p]}
              /\ UNCHANGED &lt;&lt;proposals, decisions&gt;&gt;
</code></pre></div></div>

<p>Again, it is the logical conjunct of four statements that say that in a transition \(S_i \rightarrow S_{i+1}\) where process \(p\) proposes a value:</p>
<ol>
  <li>In \(S_i\), \(states(p) \mapsto ``working"\), i.e. \(p\) has not yet proposed.</li>
  <li>In \(S_{i+1}\), \(\forall q \ne p : states_{S_{i+1}}(q) \mapsto states_{S_i}(q)\) and \(states_{S_{i+1}}(p) \mapsto ``proposed"\), i.e. all states are unchanged except that of \(p\).</li>
  <li>In \(S_{i+1}\), \(pset\) now contains the value \(proposal(p)\).</li>
  <li>In \(S_{i+1}\), the functions \(proposals\) and \(decisions\) are exactly those in \(S_i\).</li>
</ol>

<p>Likewise, I express \(decide\) as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>decide(p) == /\ ~ \E r \in PCS : states[r] = "working"
             /\ states[p] = "proposed"
             /\ states' = [states EXCEPT ![p] = "decided"]
             /\ decisions[p] = -1
             /\ decisions' = [decisions EXCEPT ![p] = CHOOSE x \in pset : TRUE]
             /\ UNCHANGED &lt;&lt;proposals, pset&gt;&gt;
</code></pre></div></div>
<p>It describes the transition where process \(p\) proposes a value, and is the logical conjunct of:</p>
<ol>
  <li>In \(S_i\), there does not exist any process \(q\) such that \(states(q) \mapsto ``working"\), i.e. every process has proposed.</li>
  <li>In \(S_{i+1}\), \(\forall q \ne p : states_{S_{i+1}}(q) \mapsto states_{S_i}(q)\) and \(states_{S_{i+1}}(p) \mapsto ``decided"\), i.e. all states are unchanged except that of \(p\).</li>
  <li>
    <ol>
      <li>In \(S_i\), \(decisions(p) \mapsto -1\), i.e. \(p\) has not yet decided.</li>
    </ol>
  </li>
  <li>In \(S_{i+1}\), \(\forall q \ne p : decisions_{S_{i+1}}(q) \mapsto decisions_{S_i}(q)\) and \(decisions_{S_{i+1}}(p)\) maps to some deterministically chosen value from \(pset\). In the semantics of TLA+, this value will be the same for all processes.</li>
  <li>In \(S_{i+1}\), the functions \(proposals\) and \(decisions\) are exactly those in  \(S_i\).</li>
</ol>

<p>Finally, we can express \(next\) as simply</p>

\[next \equiv \exists p \in PCS : propose(p) \lor decide(p)\]

<p>which means that in any step \(S_i \rightarrow S_{i+1}\) there exists a process \(p\) which either proposes or decides a value. When such a process does not exist, then the protocol halts. In TLA+ syntax, that’s</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>next == \E p \in PCS : propose(p) \/ decide(p)
</code></pre></div></div>

<h2 id="checking-for-correctness">Checking for Correctness</h2>
<p>The protocol is complete. Now we want to make sure that if \(init\) and \(next\) are true on</p>

\[S_0 \rightarrow S_1 \rightarrow S_2 \rightarrow S_3 \rightarrow ...\]

<p>, then we satisfy the spec of Consensus that is <strong>validity</strong>, <strong>agreement</strong>, <strong>integrity</strong> and <strong>termination</strong>. TLA+ allows us to do exactly that by expressing these as invariants.</p>

<p>But first, we also want to make sure we did not break any type requirements by specifying the invariant \(typeOK\). I shall omit detailed descriptions as the syntax is similar to what I have described above.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>typeOK == /\ states \in [PCS -&gt; {"working", "proposed", "decided"}]
          /\ proposals \in [PCS -&gt; {0, 1}]
          /\ pset \subseteq {0, 1}
          /\ decisions \in [PCS -&gt; {-1, 0, 1}]
</code></pre></div></div>

<p>Finally, we have the invariants</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>validity == \E v \in {0, 1} : (\A p \in PCS : proposals[p] = v) 
                =&gt; \A q \in PCS : (states[q] = "decided" 
                    =&gt; decisions[q] = v)

agreement == \A p1, p2 \in PCS : ~ /\ decisions[p1] = 0
                                   /\ decisions[p2] = 1
                                  
integrity == \A p \in PCS : 
                (states[p] = "decided" 
                =&gt; \E r \in PCS : proposals[r] = decisions[p])

specOK == /\ validity
          /\ agreement
          /\ integrity
</code></pre></div></div>
<p>Here, the only syntax we have yet seen is <code class="language-plaintext highlighter-rouge">=&gt;</code>, which represents logical implication. Therefore, the menacing description for \(validity\) means, quite simply, that, if there exists a value \(v \in \{0, 1\}\) such that everyone proposed \(v\), then at any time for all processes \(q\), if \(q\) has decided, then \(q\) must have decided \(v\).</p>

<p>But what about <strong>termination</strong>? Well, termination means that the protocol halts. In TLA+, that is characterized as a form of deadlock, and TLA+ has built-in detection for that.</p>

<p>We then set up a TLA+ model as so, telling it to check the behavior described by \(init\) and \(next\) against the invariants, and for deadlock.
<img src="/assets/images/consensus/model.png" alt="" /></p>

<p>Running the model tells us what we have is correct.</p>

<h2 id="concluding-remarks">Concluding Remarks</h2>
<p>It does seem that the Consensus protocol I have described works like magic. There is no message passing, and processes spontaneously decide on a value from a global set. Yet this is the goal. As emphasized above, this is a high-level description of what Consensus <em>should</em> do, and not how it achieves it.</p>

<p>Of course, TLA+ also allows us to specify a particular implementation of Consensus, complete with message passing and the like. But the point of having done as we did here is that using TLA+, we can later check any specific implementation against this high-level specification we have written. In other words, a specific instance of Consensus is correct if it implies that the general spec is satisfied. I will explore doing that in later post.</p>

<p>Lastly, TLA+ lets us pretty-print our spec, so it looks nicely formatted in latex-style math. You can check out the pretty-printed version of this spec, and the code itself, on <a href="https://github.com/TonyZhangND/TLAplus/tree/master/Consensus">my GitHub page</a>.</p>]]></content><author><name>Tony Zhang</name><email>nudzhang@umich.edu</email></author><summary type="html"><![CDATA[Looking to dive into distributed systems research, I recently started learning how to write TLA+ specifications. TLA+ is a specification language used to mathematically describe an algorithm, and is adopted in both academia and industry. It provides functionalities to formally verify algorithm correctness. Precisely, it allows us to check if an algorithm satisfies the properties we want it to satisfy.]]></summary></entry></feed>